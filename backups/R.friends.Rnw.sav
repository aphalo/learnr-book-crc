% !Rnw root = appendix.main.Rnw

<<echo=FALSE, include=FALSE>>=
opts_chunk$set(opts_fig_wide)
opts_knit$set(concordance=TRUE)
@

\chapter{If and when R needs help}\label{chap:R:performance}

\dictum[Patrick J. Burns (1998) S Poetry. \url{http://www.burns-stat.com/documents/books/s-poetry/}]{Improving the efficiency of your S functions can be well worth some effort.\ \ldots But remember that large efficiency gains can be made by using a better algorithm, not just by coding the same algorithm better.}

\section{Aims of this chapter}

In this final chapter I highlight what in my opinion are limitations and advantages of using \langname{R} as a scripting language in data analysis, briefly describing alternative approaches that can help overcome performance bottle necks in R code.

\section{Packages used in this chapter}

<<eval=FALSE>>=
install.packages(learnrbook::pkgs_ch_performance)
@

For executing the examples listed in this chapter you need first to load the following packages from the library:

<<message=FALSE>>=
library(Rcpp)
library(inline)
# library(rPython)
library(rJava)
library(microbenchmark)
library(profr)
library(tibble)
library(ggplot2)
@

\section{R's limitations and strengths}

\subsection{Introduction to R code optimization}

Some\index{performance}\index{code!optimization} constructs like \code{for} and \code{while} loops execute slowly in \langname{R}, as they are interpreted. Byte compiling and Just-In-Time (JIT) compiling of loops (enabled by default in R >= 3.4.0) should decreases this burden. However, base R as well some packages define several \emph{apply} functions. Being compiled functions, written in \langname{C} or \langname{C++}, using apply functions instead of explicit loops \emph{can} provide an improvement in performance while keeping user's code fully written in R. Even more effective is using vectors for indexing in operations so as to eliminate iteration loops. Pre-allocating memory, rather than growing a vector or array at each iteration can help. One little known problem is related to consistency tests when `growing' data frames. If we add one by one variables to a large data frame the overhead is in many cases huge. This can be easily reduced in many cases by assembling the object as a list, and once assembled converting it into a data frame. These common cases are used as examples in this chapter.

You may ask, how can I know, where in the code is the performance bottleneck\index{code!performance}. During the early years of R, this was quite a difficult task. Nowadays, good code profiling\index{code!profiling} and code benchmarking\index{code!benchmarking} tools are availabke. With interactive profiling now also implemented in the \pgrmname{RStudio} IDE. Profiling consists in measuring how much of the total runtime is spent in different functions, or even lines of code. Benchmarking consists in timing the execution of alternative versions of some piece of code, to decide which one should preferred.

There are some rules of style\index{code!writing style}, and common sense, that should be always applied, to develop good quality program code. However, as in some cases, high performance comes at the cost of a more complex program or algorithm, optimizations should be applied only to the sections of the code that are limiting overall performance. Usually even when the requirement of high performance is known in advance, it is best to start with a simple implementation of a simple algorithm. Get this first solution working reliably, and use this as a reference both for performance and accuracy of returned results while attempting performance optimization.

The book \citetitle{Matloff2011} \autocite{Matloff2011} is very good at presenting the use of R language and how to profit from its peculiar features to write concise and efficient code.
Studying the book \citetitle{Wickham2014advanced} \autocite{Wickham2014advanced} will give you a deep understanding of the R language, its limitations and good and bad approaches to its use. If you aim at writing R packages, then \citetitle{Wickham2015} \autocite{Wickham2015} will guide you on how to write your own packages, using modern tools. Finally, any piece of software, benefits from thorough and consistent testing, and R packages and scripts are no exception. Building a set of test cases simplifies enormously code maintenance, as they help detect unintended changes in program behaviour \autocite{Wickham2015,Cotton2016}.

\section{Measuring and improving performance}

In this section we present simple ``toy'' examples of how execution speed of R code can be improved. These examples demonstrate the use of benchmarking and profiling tools and of R's built-in compiler to improve performance of R code. 

\subsection{Benchmarking}

We call benchmarking to the measurement of overall performance of a piece of code, in the examples below a statement calling a user-defined function. Benchmarking returns a single total time of execution, but as replication is normally used, several summaries derived from the repeated runs of the same code, by default 100 \code{rounds}.
The examples bellow show how different approaches to a simple computation affect performance. We use package function \code{microbenchmark} from \pkgname{microbenchmark} to assess how effective our attempts at improving performance are.

<<>>=
library(microbenchmark)
@

A function using a loop without any optimization for performance, with a fairly usual structure.

<<>>=
my.fun01 <- function(x) {
  y <- numeric()
  for (i in seq(along.with = x[-1])) {
    y[i] <- x[i] * x[i+1] / log10(123456)
  }
  y
}
@

We time it. We are mainly interested in the \emph{median} time. We use as input a numeric vector of length 1000. Be default a suitable time unit is used, but we set it to milliseconds with \code{unit = "ms"} to make sure that the same units are used in all the examples.

<<>>=
microbenchmark(my.fun01(runif(1e3)), unit = "ms")
@

\begin{playground}
  Before continuing reading, study carefully the definition of \code{my.fun01()} and try to discover way of improving its performance.
\end{playground}

One simple step is to reserve storage space for the result vector \code{y} ahead of the iteration through a loop. We
simply replace \code{y <- numeric()} by \code{y <- numeric(length(x))}.
<<>>=
my.fun02 <- function(x) {
  y <- numeric(length(x))
  for (i in seq(along.with = x[-1])) {
    y[i] <- x[i] * x[i+1] / log10(123456)
  }
  y
}
@

Benchmarking the new function shows that run time has decreased by 40\%.

<<>>=
microbenchmark(my.fun02(runif(1e3)), unit = "ms")
@

Within the loop we have the calculation of the logarithm of a constant, a value that does not change from iteration to iteration, in technical terms, a \emph{loop invariant computation}. We move this invariant calculation out of the loop.

<<>>=
my.fun03 <- function(x) {
  y <- numeric(length(x))
  k <- log10(123456)
  for (i in seq(along.with = x[-1])) {
    y[i] <- x[i] * x[i+1] / k
  }
  y
}
@

Benchmarking reveals that runtime is now decreased by almost 70\% in this step, and by more than 80\% from our starting point.

<<>>=
microbenchmark(my.fun03(runif(1e3)), unit = "ms")
@

However, we can use subscripting with vectors and avoid using iteration in an explicit loop.

<<>>=
my.fun04 <- function(x) {
  i <- seq(along.with = x[-1])
  x[i] * x[i+1] / log10(123456)
}
@

Now execution is really fast! We have decreased runtime to 8\% of the starting point, a decrease of 92\%.

<<>>=
microbenchmark(my.fun04(runif(1e3)), unit = "ms")
@

\begin{playground}
  The examples above show that in many cases performance can be improved a lot without recourse to complex code. In fact our optimized function is a lot simpler than the initial one. Having read the book as far as this section, you must surely have some code of your own that could be optimized for performance. Make use of the ideas in the section to improve your own code.
\end{playground}

\begin{playground}
  The example code used in this section needs also to be tested at boundary conditions. Try with any of the functions to run the following code.
<<eval=FALSE>>=
my.fun01(1)
my.fun01(numeric(0))
my.fun01(NA)
my.fun01(NULL)
@

Repeat these tests for all the four functions. Do they all return the same values in each of these cases?

Finally validate the output of \code{my.fun04()} against \code{my.fun01()} with numeric arguments of different lengths.
\end{playground}

\subsection{Profiling}

Profiling is the estimation of how much different parts of the code contribute to the total execution time. For example a fine grained profiling would be based on individual statements in the code. We will here use a different example.

We check that our code is correct and returns the expected value, in this case a 100$\times$100 data frame.

<<>>=
my.fun11 <- function(row = 100, col = 100) {
    df <- data.frame(1:row)
    for (i in 2:col)
      df <- cbind(df, 1:row)
  df
}
@

We check the total time as in the previous section, and at 15~ms seems quite slow.

<<>>=
microbenchmark(my.fun11(row = 100, col = 100), unit = "ms")
@

In the next two code chunks we find out whether adding columns or rows is more expensive in run time.

<<>>=
microbenchmark(my.fun11(row = 200, col = 100), unit = "ms")
@

<<>>=
microbenchmark(my.fun11(row = 100, col = 200), unit = "ms")
@

We now know that what is expensive is to add columns. We look into this problem by profiling the code to find where most time is being spent. We need to adjust the sampling interval to match how many statements has the code under test and how long it takes to run. We can use the plot as a guide, if resolution is too little with too few levels and functions we need to decrease the interval, while if the plot looks like an unintelligible comb, the interval needs to be increases.

<<>>=
prof.df <- profr(my.fun11(row = 100, col = 100), interval = 0.001)

ggplot(prof.df)
@

\begin{playground}
Modify the value passed as argument to parameter \code{interval}, and find a good value that gives more detail than the one used above. Be aware that profiling is done by sampling, and the output may vary between runs.
\end{playground}

The problem seems to be with data frame and column-binding. We try a different approach to building a data frame: building first a list and converting it into a data frame once assembled.

<<>>=
my.fun12 <- function(row = 100, col = 100) {
lst <- list()
    for (i in 1:col)
      lst <- c(lst, list(1:row))
    as.data.frame(lst)
}
@

We achieve a 40\% reduction in execution time.

<<>>=
microbenchmark(my.fun12(row = 100, col = 100), unit = "ms")
@

An optimization specific to this case is to use \code{rep()} to create a long vector, convert it into a matrix and the matrix into a data frame.

<<>>=
my.fun13 <- function(row = 100, col = 100) {
  mtx <- matrix(rep(1:row, col), ncol = col)
  as.data.frame(mtx)
}
@

A round about but very effective way of improving performance with a runtime that is only 1.6\% of the starting one, or a reduction in execution time by more than 98\%.

<<>>=
microbenchmark(my.fun13(row = 100, col = 100), unit = "ms")
@

We may still want to know where time is being spent in the optimized version of the function.

<<>>=
prof13.df <- profr(my.fun13(row = 100, col = 100), interval = 0.00003)

ggplot(prof13.df)
@

Can we still improve it?

<<>>=
my.fun14 <- function(row = 100, col = 100) {
  mtx <- matrix(rep(1:row, col), ncol = col)
  as_tibble(mtx)
}
@

Yes, indeed, using \code{as\_tibble()} instead of \code{as.data.frame()} halves runtime, and we have by now decreased execution time by over two orders of magnitude compared to the first version of our function.

<<>>=
microbenchmark(my.fun14(row = 100, col = 100), unit = "ms")
@

\begin{playground}
We have gone very far in optimizing the function. In this last version the function returns a \code{tibble} instead of a \code{data.frame}. This can be expected to affect the performance of different operations, from indexing and computations to summaries. Use bench marking to assess these differences, both for cases with substantially more columns than rows, and more rows than columns. Think carefully a test case that makes heave use of indexing, calculations combining several columns, and sorting.
\end{playground}

Additional profiling tools such as those provided by package \pkgname{proftools} can be useful when dealing with more complex pieces of code.

\subsection{Compiling R functions}

Although the current version of R uses by default the compiler quite frequently, we will demonstrate its manual use.
We can see if a function is compiled, by printing it and looking if it contains a pointer to byte code.

To test what speed-up compiling can achieve for this small function we switch-off default compiling momentarily with function \code{enaleJIT()}---JIT is an abbreviation for Just In Time compiler. Possible values of \code{levels} range from 0 to 3. Zero disables the compiler, while 1, 2 and 3 indicate use of the compiler by default in an increasing number of situations.

We define, using a different name, the same function as earlier, and we check that it has not been compiled. Then we compile it. 

<<>>=
old.level <- compiler::enableJIT(level = 0L)

my.fun11nc <- function(row = 100, col = 100) {
    df <- data.frame(1:row)
    for (i in 2:col)
      df <- cbind(df, 1:row)
  df
}

my.fun11nc

my.fun11c <- compiler::cmpfun(my.fun11nc)

my.fun11c

microbenchmark(my.fun11nc(row = 100, col = 100), unit = "ms")
microbenchmark(my.fun11c(row = 100, col = 100), unit = "ms")

compiler::enableJIT(level = old.level)
@

For this small function compilation yields only a small reduction in execution time.

\section{R is great, but not always best}

\subsection{Using the best tool for each job}

Frequently, optimizing \langname{R} code for performance can yield more than an order of magnitude decrease in runtime. In many cases this is enough, and the most cost-effective solution. There are both packages and functions in base R, that if properly used can make a huge difference in performance. In addition, efforts in recent years to optimize the overall performance of R itself have been successful. Some of the packages with enhanced performance have been described in earlier chapters, as they are easy enough to use and have also an easy to learn user interfaces. Other packages like \pkgname{data.table} although achieving very fast execution, incur the cost of using a user interface and having a behaviour alien to the ``normal way of working'' with R.

Sometimes, the best available tools for a certain job have not been implemented in R but are available in other languages. Alternatively, the algorithms or the size of the data are such that performance is poor when implemented in the R language, and can improved using a different language.

One extremely important feature leading to the success of \langname{R} is extensibility\index{R!extensibility}. Not only by writing packages in R itself, but by allowing the development of packages containing functions written in other computer languages. The beauty of the package loading mechanism, is that even if \pgrmname{R} itself is written in \langname{C}, and compiled into an executable, packages containing interpreted \langname{R} code, and also compiled \langname{C}, \langname{C++}, \langname{FORTRAN}, or other languages, or calling libraries written in \langname{Java}, \langname{Python}, etc. can be loaded and unloaded at runtime.

Most common reasons for using other programming languages, are the availability of libraries written in \langname{FORTRAN}, \langname{C} and \langname{C++} that are well tested and optimized for performance. This is frequently the case for numerical calculations and time-consuming data manipulations like image analysis. In such cases the R code in packages is just a wrapper (or ``glue'') to allow the functions in the library to be called from R.

In other cases we may diagnose a performance bottleneck, and decide to write a few functions within a package otherwise written in R, in a compiled language like \langname{C++}. In such cases is a good idea to use bench marking to compare implementations, as the use of a different language does not necessarily provide a worthwhile performance enhancement. The reason behind this is that different languages do not always store data in computer memory in the same format. Differences among languages can add overhead to function calls across them, specially when they execute very quickly when called from R and/or when large amounts of data need to be shuffled back and forth between R and functions written in other languages. The R proitself is written in the \langname{C} language. 

\subsection{C++}

Nowadays, thanks to package \pkgname{Rcpp}, using \langname{C++} code mixed with R language, is fairly simple \autocite{Eddelbuettel2013}. This package does not only provide R code, but a \langname{C++} header file with macro definitions that reduces the writing of the necessary ``glue'' code to the use of a simple macro in the \langname{C++} code. Although, this mechanism is most frequently used as a component packages, it is also possible to define a function written in \langname{C++} at the R console, or in a simple user's script. Of course for these to work all the tools needed to build R packages from source are needed, including a suitable compiler and linker.

An example taken from the \pkgname{Rcpp} documentation follows. This is an example of how one would define a function during an interactive session at the R console, or in a simple script. When writing a package, one would write a separate source file for the function, include the \code{rcpp.h} header and use the \langname{C++} macros to build the R code side. Using \langname{C++} inline requires package \pkgname{inline} to be loaded in addition to \pkgname{Rcpp}.

First we save the source code for the function written in \langname{C++}, taking advantage of types and templates defined in the \code{Rccp.h} header file.

<<rcpp-01>>=
src <- '
 Rcpp::NumericVector xa(a);
 Rcpp::NumericVector xb(b);
 int n_xa = xa.size(), n_xb = xb.size();

 Rcpp::NumericVector xab(n_xa + n_xb - 1);
 for (int i = 0; i < n_xa; i++)
 for (int j = 0; j < n_xb; j++)
 xab[i + j] += xa[i] * xb[j];
 return xab;
'
@

The second step is to compile and load the function, in a way that it can be called from R code and indistinguishable from a function defined in R itself.

<<rcpp-02>>=
fun <- cxxfunction(signature(a = "numeric", b = "numeric"), src, plugin = "Rcpp")
@

We can now use it as any other R function.

<<rcpp-03>>=
fun(1:3, 1:4)
@

As we will see below, this is not the case when calling Java and Python functions, cases where although the integration is relatively tight, special syntax is used when calling the ``foreign'' functions and/or methods. The advantage of Rcpp in this respect is very significant, as we can define functions that have exactly the same argument signature, use the same syntax and behave in the same way, using either the R or \langname{C++} language. This means that at any point during development of a package a function defined in R can be replaced by an equivalent function defined in \langname{C++}, or vice versa, with absolutely no impact on user's code, except possibly by faster execution of the \langname{C++} version.

\subsection{FORTRAN and C}

In the case of \langname{FORTRAN} and \langname{C}, the process is less automated in the R code needed to call the compiled functions needs to be explicitly written (See \emph{Writing R Extensions} in the R documentation, for up-to-date details). Once written, the building and installation of the package is automatic. This is the way how many existing libraries are called from within R and R packages.

\subsection{Python}

Package \pkgname{rPython} allows calling \langname{Python} functions and methods from R code. Currently this package is not available under MS-Windows.

Example taken from the package description (not run).

<<rpython-01,eval=FALSE>>=
python.call( "len", 1:3 )
a <- 1:4
b <- 5:8
python.exec( "def concat(a,b): return a+b" )
python.call( "concat", a, b)
@

It is also possible to call R functions from \langname{Python}. However, this is outside the scope of this book.

\subsection{Java}

Although \langname{Java} compilers exist, most frequently Java programs are compiled into intermediate byte code and this is interpreted, and usually the interpreter includes a JIT compiler. For calling \langname{Java} functions or accessing Java objects from R code, the solution is to use package \pkgname{rJava}. One important point to remember is that the Java Development Environment must be installed for this package to work. The usually installed runtime is not enough.

We need first to start the Java Virtual Machine (the byte-code interpreter).

<<rjava-01>>=
.jinit()
@

The code that follows is not that clear, and merits some explanation.

We first create a \langname{Java} array from inside R.

<<rjava-02>>=
a <- .jarray( list(
                   .jnew( "java/awt/Point", 10L, 10L ),
                   .jnew( "java/awt/Point", 30L, 30L )
                   ) )
print(a)
mode(a)
class(a)
str(a)
@

Then we use base R's function \Rfunction{lapply()} to apply a user-defined R function to the elements of the Java array, obtaining as returned value an R array.

<<rjava-03>>=
b <- sapply(a,
            function(point){
              with(point, {
                (x + y )^2
              } )
            })
print(b)
mode(b)
class(b)
str(b)
@

Although more cumbersome than in the case of \pkgname{Rcpp} one can manually write wrapper code to hide the special syntax and object types from users.

It is also possible to call R functions from within a \langname{Java} program. This is outside the scope of this book.

\subsection{sh, bash}

The\index{command shell}\index{sh}\index{bash} operating system shell can be accessed from within R and the output from programs and shell scripts returned to the R session. This is useful, for example for pre-processing raw data files with tools like \langname{AWK} or \langname{Perl} scripts. The problem with this approach is that when it is used, the R script cannot run portably across operating systems, or in the absence of the tools or sh or bash scripts. Except for code that will never be reused (i.e.\ it is used once and discarded) it is preferable to use R's built-in commands whenever possible, or if shell scripts are used, to make the shell script the master script from within which the R scripts are called, rather than the other way around. The reason for this is mainly making clear the developer's intention: that the code as a whole will be run in a given operating system using a certain set of tools, rather hiding shell calls inside the R script. In other words, keep the least portable bits in full view.

\section{Web pages, and interactive interfaces}

There is a lot that could be written on the using R to create web pages, interactive widgets and gadgets for both local and remote use. This is an area currently under intense development. One example is the \pkgname{Shiny} package and Shiny server \url{https://shiny.rstudio.com/}. This package allows the creation of interactive displays to be viewed through any web browser.

\sloppy
There are other packages for generating both static and interactive graphics in formats suitable for on-line display, as well as package \pkgname{knitr} \url{https://yihui.name/knitr/} used for writing the present book, which when using R Markdown for markup (with package \pkgname{rmarkdown} \url{http://rmarkdown.rstudio.com} or \pkgname{Bookdown} \url{https://bookdown.org/} can output self-contained HTML files in addition to RTF and PDF formats.

<<>>=
try(detach(package:ggplot2))
try(detach(package:tibble))
try(detach(package:profr))
try(detach(package:microbenchmark))
try(detach(package:rJava))
# try(detach(package:rPython))
try(detach(package:inline))
try(detach(package:Rcpp))
@


