% !Rnw root = appendix.main.Rnw

<<echo=FALSE, include=FALSE>>=
opts_chunk$set(opts_fig_wide)
opts_knit$set(concordance=TRUE)
opts_knit$set(unnamed.chunk.label = 'friends-chunk')
@

\chapter{If and when \Rlang needs help}\label{chap:R:performance}

\begin{VF}
Improving the efficiency of your S functions can be well worth some effort.\ \ldots But remember that large efficiency gains can be made by using a better algorithm, not just by coding the same algorithm better.

\VA{Patrick J. Burns (1998)}{S Poetry}
\end{VF}

%\dictum[Patrick J. Burns (1998) S Poetry. \url{http://www.burns-stat.com/documents/books/s-poetry/}]{Improving the efficiency of your S functions can be well worth some effort.\ \ldots But remember that large efficiency gains can be made by using a better algorithm, not just by coding the same algorithm better.}

\section{Aims of this chapter}

In this final chapter I highlight what in my opinion are limitations and advantages of using \Rlang as a scripting and programming language for data analysis, briefly describing alternative approaches that can help overcome performance bottle necks in \Rlang code. A few books cover this subject from different perspectives, including \citebooktitle{Wickham2015} \autocite{Wickham2015}, \citebooktitle{Wickham2019} \autocite{Wickham2019}, \citebooktitle{Matloff2011} \autocite{Matloff2011}, \citebooktitle{Chambers2016} \autocite{Chambers2016} from a practical programming perspective, and, \citebooktitle{Burns1998} \autocite{Burns1998} and \citebooktitle{Burns2011} \autocite{Burns2011} from a more conceptual and language design oriented perspective.

\section{Packages used in this chapter}

<<eval=FALSE>>=
install.packages(learnrbook::pkgs_ch_performance)
@

For executing the examples listed in this chapter you need first to load the following packages from the library:

<<message=FALSE>>=
library(Rcpp)
library(inline)
# library(rPython)
library(rJava)
library(microbenchmark)
library(profr)
library(tibble)
library(ggplot2)
@

\begin{explainbox}
\textbf{Interpreters and compilers}\index{compiler}\index{interpreter}\index{byte compiler} Computer programs and scripts are nowadays almost always written in a high level language that is readable to humans, and that relies on a grammar much more complex than that understood by the hardware processor chip in the computer or device. Consequently one or more translation steps are needed. An interpreter, translates user code at the time of execution, and consequently parts of the code that are executed repeatedly are translated multiple times. A native compiler translates the user code into machine code in a separate step, and the compiled machine code can be stored and executed itself as many times as needed. On the other hand, compiled code can be executed only on a given hardware (processor, or processors from a given family). A byte-code compiler, translates user code into an intermediate representation, which cannot be directly executed by any hardware, and which is independent of the hardware architecture, but easier/faster to translate into machine code. This second interpreter is called a ``virtual machine'', as it is not dependent on a real hardware processor architecture.

An interpreter adds flexibility and makes interactive use possible, but results in slower execution compared to compiled executables. Nowadays, byte compiling is part of the \Rpgrm program, and used by default in some situations or under user control. Just-in-time (JIT) compiling is a relatively new feature in \Rpgrm, and consists in compiling on-the-fly code that is repeatedly evaluated within a single run of a script.

Functions or subroutines that have been compiled to machine code can be called from within \Rpgrm, but currently not written in the \Rlang language itself, as no native compiler exists for the \Rlang language. It is common to call from within \Rpgrm code, compiled functions or use whole libraries coded in languages such a \langname{C}, \langname{C++} and \langname{FORTRAN} when maximum execution speed is needed. The calls are normally done from within an \Rpgrm package, so that they appear to the user not different any other \Rlang function. Functions and libraries written in other interpreted and/or byte-compiled languages like \langname{Java} and \langname{Python} can also be called from \Rpgrm.

In addition, \Rpgrm exposes a programming interface (API) and many \Rpgrm functions can be called from within programs or scripts written in other languages such a \langname{Python} and \langname{Java}, also database systems and work sheets. This flexibility is one of the reasons behind \Rpgrm's popularity.
\end{explainbox}


\section{R's limitations and strengths}

\subsection{Introduction to \Rlang code optimization}

Some\index{performance}\index{code!optimization} constructs like \code{for} and \code{while} loops execute slowly in \Rlang, as they are interpreted. Byte compiling and Just-In-Time (JIT) compiling of loops (enabled by default in R >= 3.4.0) should decreases this burden. However, base \Rlang as well some packages define several \emph{apply} functions. Being compiled functions, written in \langname{C} or \langname{C++}, using apply functions instead of explicit loops \emph{can} provide an improvement in performance while keeping user's code fully written in R. Even more effective is using vectors for indexing in operations so as to eliminate iteration loops. Pre-allocating memory, rather than growing a vector or array at each iteration can help. One little known problem is related to consistency tests when `growing' data frames. If we add one by one variables to a large data frame the overhead is in many cases huge. This can be easily reduced in many cases by assembling the object as a list, and once assembled converting it into a data frame. These common cases are used as examples in this chapter.

You may ask, how can I know, where in the code is the performance bottleneck\index{code!performance}. During the early years of R, this was quite a difficult task. Nowadays, good code profiling\index{code!profiling} and code benchmarking\index{code!benchmarking} tools are available. With interactive profiling now also implemented in the \RStudio IDE. Profiling consists in measuring how much of the total runtime is spent in different functions, or even lines of code. Benchmarking consists in timing the execution of alternative versions of some piece of code, to decide which one should preferred.

There are some rules of style\index{code!writing style}, and common sense, that should be always applied, to develop good quality program code. However, as in some cases, high performance comes at the cost of a more complex program or algorithm, optimizations should be applied only to the sections of the code that are limiting overall performance. Usually even when the requirement of high performance is known in advance, it is best to start with a simple implementation of a simple algorithm. Get this first solution working reliably, and use this as a reference both for performance and accuracy of returned results while attempting performance optimization.

The book \citebooktitle{Matloff2011} \autocite{Matloff2011} is very good at presenting the use of \Rlang language and how to profit from its peculiar features to write concise and efficient code.
Studying the book \citebooktitle{Wickham2019} \autocite{Wickham2019} will give you a deep understanding of the \Rlang language, its limitations and good and bad approaches to its use. If you aim at writing \Rlang packages, then \citebooktitle{Wickham2015} \autocite{Wickham2015} will guide you on how to write your own packages, using modern tools. Finally, any piece of software, benefits from thorough and consistent testing, and \Rlang packages and scripts are no exception. Building a set of test cases simplifies enormously code maintenance, as they help detect unintended changes in program behaviour \autocite{Wickham2015,Cotton2016}.

\section{Measuring and improving performance}\label{sec:performance:tuning}

In this section we present simple ``toy'' examples of how execution speed of \Rlang code can be improved. These examples demonstrate the use of benchmarking and profiling tools and of \Rlang built-in compiler to improve performance of \Rlang code.

\subsection{Benchmarking}
\index{code benchmarking|(}
We call benchmarking to the measurement of overall performance of a piece of code, in the examples below a statement calling a user-defined function. Benchmarking returns a single total time of execution, but as replication is normally used, several summaries derived from the repeated runs of the same code, by default 100 \code{rounds}.
The examples below show how different approaches to a simple computation affect performance. We use package function \code{microbenchmark} from \pkgname{microbenchmark} to assess how effective our attempts at improving performance are.

<<>>=
library(microbenchmark)
@

A function using a loop without any optimization for performance, with a fairly usual structure.

<<microbenchmark-01>>=
my.fun01 <- function(x) {
  y <- numeric()
  for (i in seq(along.with = x[-1])) {
    y[i] <- x[i] * x[i+1] / log10(123456)
  }
  y
}
@

We time it. We are mainly interested in the \emph{median} time. We use as input a numeric vector of length 1000. Be default a suitable time unit is used, but we set it to milliseconds with \code{unit = "ms"} to make sure that the same units are used in all the examples.

<<microbenchmark-02>>=
microbenchmark(my.fun01(runif(1e3)), unit = "ms")
@

\begin{playground}
  Before continuing reading, study carefully the definition of \code{my.fun01()} and try to discover way of improving its performance.
\end{playground}

One simple step is to reserve storage space for the result vector \code{y} ahead of the iteration through a loop. We
simply replace \code{y <- numeric()} by \code{y <- numeric(length(x))}.
<<microbenchmark-03>>=
my.fun02 <- function(x) {
  y <- numeric(length(x))
  for (i in seq(along.with = x[-1])) {
    y[i] <- x[i] * x[i+1] / log10(123456)
  }
  y
}
@

Benchmarking the new function shows that run time has decreased by 40\%.

<<microbenchmark-04>>=
microbenchmark(my.fun02(runif(1e3)), unit = "ms")
@

Within the loop we have the calculation of the logarithm of a constant, a value that does not change from iteration to iteration, in technical terms, a \emph{loop invariant computation}. We move this invariant calculation out of the loop.

<<microbenchmark-05>>=
my.fun03 <- function(x) {
  y <- numeric(length(x))
  k <- log10(123456)
  for (i in seq(along.with = x[-1])) {
    y[i] <- x[i] * x[i+1] / k
  }
  y
}
@

Benchmarking reveals that runtime is now decreased by almost 70\% in this step, and by more than 80\% from our starting point.

<<microbenchmark-06>>=
microbenchmark(my.fun03(runif(1e3)), unit = "ms")
@

However, we can use subscripting with vectors and avoid using iteration in an explicit loop.

<<microbenchmark-07>>=
my.fun04 <- function(x) {
  i <- seq(along.with = x[-1])
  x[i] * x[i+1] / log10(123456)
}
@

Now execution is really fast! We have decreased runtime to 8\% of the starting point, a decrease of 92\%.

<<microbenchmark-08>>=
microbenchmark(my.fun04(runif(1e3)), unit = "ms")
@

\begin{playground}
a) Benchmark the first three code chunks from section \ref{sec:nested:loops} starting at page \pageref{sec:nested:loops}. How big is the difference in execution time?
b) Rerun the same benchmarks with 10, 100 and 1000 outer iterations. How do the differences among approaches depend on the number of iterations?
\end{playground}

\begin{playground}
  The examples above show that in many cases performance can be improved a lot without recourse to complex code. In fact the code for our optimized function is a lot simpler than the initial one. Having read the book as far as this section, you must surely have some code of your own that could be optimized for performance. Make use of the ideas in this section to improve your own code.
\end{playground}

\begin{advplayground}
The example code used in this section needs also to be tested at boundary conditions. Try with any of the functions to run the following code.
<<eval=FALSE>>=
my.fun01(1)
my.fun01(numeric(0))
my.fun01(NA)
my.fun01(NULL)
@

Repeat these tests for all the four functions. Do they all return the same values in each of these cases?

Finally validate the output of \code{my.fun04()} against \code{my.fun01()} with numeric arguments of different lengths.
\end{advplayground}

\index{code benchmarking|)}


\subsection{Profiling}
\index{code profiling|(}

Profiling is the estimation of how much different parts of the code contribute to the total execution time. For example a fine grained profiling would be based on individual statements in the code. We will here use a different example.

We check that our code is correct and returns the expected value, in this case a 100$\times$100 data frame.

<<profiling-01>>=
my.fun11 <- function(row = 100, col = 100) {
    df <- data.frame(1:row)
    for (i in 2:col)
      df <- cbind(df, 1:row)
  df
}
@

We check the total time as in the previous section, and at 15~ms seems quite slow. One interesting piece of information is how the time spent increases with increasing size of the problem. There are two aspects to this: the shape of the curve, e.g.\ linear or exponential, and the problem-size-independent time overhead.

<<profiling-02>>=
microbenchmark(my.fun11(row = 10, col = 10), unit = "ms")
microbenchmark(my.fun11(row = 100, col = 100), unit = "ms")
microbenchmark(my.fun11(row = 1000, col = 1000), unit = "ms")
@

In the next two code chunks we find out whether adding columns or rows is more expensive in terms of run time.

<<profiling-03>>=
microbenchmark(my.fun11(row = 200, col = 100), unit = "ms")
@

<<profiling-04>>=
microbenchmark(my.fun11(row = 100, col = 200), unit = "ms")
@

We now know that what is expensive is to add columns. We look into this problem by profiling the code to find where most time is being spent. We need to adjust the sampling interval to match how many statements has the code under test and how long it takes to run them. We can use the plot as a guide, if resolution is too little with too few levels and functions we need to decrease the interval, while if the plot looks like an unintelligible comb, the interval needs to be increases.

<<profiling-04a>>=
microbenchmark(my.fun11(row = 200, col = 200), unit = "ms")
@

<<profiling-05>>=
prof.df <- profr(my.fun11(row = 200, col = 200), interval = 0.001)

ggplot(prof.df)
@

\begin{playground}
Modify the value passed as argument to parameter \code{interval}, and find a good value that gives more detail than the one used above. Be aware that profiling is done by sampling, and the output may vary between runs.
\end{playground}

The problem seems to be with data frame and column-binding. We try a different approach to building a data frame: building first a list and converting it into a data frame once assembled.

<<profiling-06>>=
my.fun12 <- function(row = 200, col = 200) {
lst <- list()
    for (i in 1:col)
      lst <- c(lst, list(1:row))
    as.data.frame(lst)
}
@

We achieve a 40\% reduction in execution time.

<<profiling-07>>=
microbenchmark(my.fun12(row = 200, col = 200), unit = "ms")
@

An optimization specific to this case---taking advantage that all values are numeric---is to use \code{rep()} to create a long vector, convert it into a matrix and the matrix into a data frame.

<<profiling-08>>=
my.fun13 <- function(row = 200, col = 200) {
  mtx <- matrix(rep(1:row, col), ncol = col)
  as.data.frame(mtx)
}
@

A round about but very effective way of improving performance with a runtime that is only 1.6\% of the starting one, or a reduction in execution time by more than 98\%.

<<profiling-09>>=
microbenchmark(my.fun13(row = 200, col = 200), unit = "ms")
@

Unless we consider the current version fast enough, we will want to know where time is being spent in the improved version of the function.
As the code runs now a lot faster it helps to increase the size of the problem for profiling.

<<profiling-11>>=
prof13.df <- profr(my.fun13(row = 2000, col = 2000), interval = 0.001)

ggplot(prof13.df)
@

Can we still improve it?

<<profiling-12>>=
my.fun14 <- function(row = 200, col = 200) {
  mtx <- matrix(rep(1:row, col), ncol = col)
  as_tibble(mtx)
}
@

Yes, indeed, using \code{as\_tibble()} instead of \code{as.data.frame()} halves runtime, and we have by now decreased execution time by over two orders of magnitude compared to the first version of our function.

<<profiling-13>>=
microbenchmark(my.fun14(row = 200, col = 200), unit = "ms")
@

\begin{playground}
Investigate how the size of the problem affects this heavily optimized code, as it was done with the initial version of the function at the start of this section.
\end{playground}

\begin{playground}
We have gone very far in optimizing the function. In this last version the function returns a \code{tibble} instead of a \code{data.frame}. This can be expected to affect the performance of different operations, from indexing and computations to summaries when applied to the returned object. Use bench marking to assess these differences, both for cases with substantially more columns than rows, and more rows than columns. Think carefully a test case that makes heavy use of indexing, calculations combining several columns, and sorting.
\end{playground}

\begin{playground}
Profile, using \code{profvis::profvis()} or \code{proftools::profr()} the first three code chunks from section \ref{sec:nested:loops} starting at page \pageref{sec:nested:loops}. In which functions is most of the execution time spent? What are the differences between the three implementations?
\end{playground}

Profiling tools such as those provided by package \pkgname{proftools} can be useful when dealing with both simple and complex pieces of code. Furthermore, package \pkgname{profvis} provides an interactive user interface within RStudio, which makes it extremely easy to navigate to the code sections from the plot displaying the corresponding timings.

\index{code profiling|)}

\subsection{Compiling \Rlang functions}
\index{R compiler|(}

Although the current version of \Rlang uses by default the compiler quite frequently, we will demonstrate its manual use.
We can see if a function is compiled, by printing it and looking if it contains a pointer to byte code.

To test what speed-up compiling can achieve for this small function we switch-off default compiling momentarily with function \code{enaleJIT()}---JIT is an abbreviation for Just In Time compiler. Possible values of \code{levels} range from 0 to 3. Zero disables the compiler, while 1, 2 and 3 indicate use of the compiler by default in an increasing number of situations.

We define, using a different name, the same function as earlier, and we check that it has not been compiled. Then we compile it.

<<compiler-01>>=
old.level <- compiler::enableJIT(level = 0L)

my.fun11nc <- function(row = 100, col = 100) {
    df <- data.frame(1:row)
    for (i in 2:col)
      df <- cbind(df, 1:row)
  df
}

my.fun11nc

my.fun11c <- compiler::cmpfun(my.fun11nc)

my.fun11c

microbenchmark(my.fun11nc(row = 100, col = 100), unit = "ms")
microbenchmark(my.fun11c(row = 100, col = 100), unit = "ms")

compiler::enableJIT(level = old.level)
@

For this small function compilation yields only a small reduction in execution time.
\index{R compiler|)}

\section{R is great, but not always best}

\subsection{Using the best tool for each job}

Frequently, optimizing \Rlang code for performance can yield more than an order of magnitude decrease in runtime. In many cases this is enough, and the most cost-effective solution. There are both packages and functions in base R, that if properly used can make a huge difference in performance. In addition, efforts in recent years to optimize the overall performance of \Rlang itself have been successful. Some of the packages with enhanced performance have been described in earlier chapters, as they are easy enough to use and have also an easy to learn user interfaces. Other packages like \pkgname{data.table} although achieving very fast execution, incur the cost of using a user interface and having a behaviour alien to the ``normal way of working'' with R.

Sometimes, the best available tools for a certain job have not been implemented in \Rlang but are available in other languages. Alternatively, the algorithms or the size of the data are such that performance is poor when implemented in the \Rlang language, and can improved using a different language.

One extremely important feature leading to the success of \Rlang is extensibility\index{R!extensibility}. Not only by writing packages in \Rlang itself, but by allowing the development of packages containing functions written in other computer languages. The beauty of the package loading mechanism, is that even if \Rpgrm itself is written in \langname{C}, and compiled into an executable, packages containing interpreted \Rlang code, and also compiled \langname{C}, \langname{C++}, \langname{FORTRAN}, or other languages, or calling libraries written in \langname{Java}, \langname{Python}, etc.\ can be loaded and unloaded at runtime.

Most common reasons for using other programming languages, are the availability of libraries written in \langname{FORTRAN}, \langname{C} and \langname{C++} that are well tested and optimized for performance. This is frequently the case for numerical calculations and time-consuming data manipulations like image analysis. In such cases the \Rlang code in packages is just a wrapper (or ``glue'') to allow the functions in the library to be called from R.

In other cases we may diagnose a performance bottleneck, and decide to write a few functions within a package otherwise written in R, in a compiled language like \langname{C++}. In such cases is a good idea to use bench marking to compare implementations, as the use of a different language does not necessarily provide a worthwhile performance enhancement. The reason behind this is that different languages do not always store data in computer memory in the same format. Differences among languages can add overhead to function calls across them, specially when they execute very quickly when called from \Rlang and/or when large amounts of data need to be shuffled back and forth between \Rlang and functions written in other languages. The \Rlang program itself is written in the \langname{C} language.

\subsection{C++}

Nowadays, thanks to package \pkgname{Rcpp}, using \langname{C++} code mixed with \Rlang language, is fairly simple \autocite{Eddelbuettel2013}. This package does not only provide \Rlang code, but a \langname{C++} header file with macro definitions that reduces the writing of the necessary ``glue'' code to the use of a simple macro in the \langname{C++} code. Although, this mechanism is most frequently used as a component packages, it is also possible to define a function written in \langname{C++} at the \Rlang console, or in a simple user's script. Of course for these to work all the tools needed to build \Rlang packages from source are needed, including a suitable compiler and linker.

An example taken from the \pkgname{Rcpp} documentation follows. This is an example of how one would define a function during an interactive session at the \Rlang console, or in a simple script. When writing a package, one would write a separate source file for the function, include the \code{rcpp.h} header and use the \langname{C++} macros to build the \Rlang code side. Using \langname{C++} inline requires package \pkgname{inline} to be loaded in addition to \pkgname{Rcpp}.

First we save the source code for the function written in \langname{C++}, taking advantage of types and templates defined in the \code{Rccp.h} header file.

<<rcpp-01>>=
src <- '
 Rcpp::NumericVector xa(a);
 Rcpp::NumericVector xb(b);
 int n_xa = xa.size(), n_xb = xb.size();

 Rcpp::NumericVector xab(n_xa + n_xb - 1);
 for (int i = 0; i < n_xa; i++)
 for (int j = 0; j < n_xb; j++)
 xab[i + j] += xa[i] * xb[j];
 return xab;
'
@

The second step is to compile and load the function, in a way that it can be called from \Rlang code and indistinguishable from a function defined in \Rlang itself.

<<rcpp-02>>=
# need to check maximum number of dll in Windows!
# and also compare to previous version of Rcpp
fun <- cxxfunction(signature(a = "numeric", b = "numeric"), src, plugin = "Rcpp")
@

We can now use it as any other \Rlang function.

<<rcpp-03>>=
fun(1:3, 1:4)
@

As we will see below, this is not the case when calling Java and Python functions, cases where although the integration is relatively tight, special syntax is used when calling the ``foreign'' functions and/or methods. The advantage of Rcpp in this respect is very significant, as we can define functions that have exactly the same argument signature, use the same syntax and behave in the same way, using either the \Rlang or \langname{C++} language. This means that at any point during development of a package a function defined in \Rlang can be replaced by an equivalent function defined in \langname{C++}, or vice versa, with absolutely no impact on user's code, except possibly for faster execution of the \langname{C++} version.

\subsection{FORTRAN and C}

In the case of \langname{FORTRAN} and \langname{C}, the process is less automated as the \Rlang code needed to call the compiled functions needs to be explicitly written (See \emph{Writing R Extensions} in the \Rlang documentation, for up-to-date details). Once written, the building and installation of the package is automatic. This is the way how many existing libraries are called from within \Rlang and \Rlang packages.

\subsection{Python}

Package \pkgname{rPython} allows calling \langname{Python} functions and methods from \Rlang code. Currently this package is not available under MS-Windows.

Example taken from the package description (not run).

<<rpython-01,eval=FALSE>>=
python.call( "len", 1:3 )
a <- 1:4
b <- 5:8
python.exec( "def concat(a,b): return a+b" )
python.call( "concat", a, b)
@

It is also possible to call \Rlang functions from \langname{Python}. However, this is outside the scope of this book.

\subsection{Java}

Although native \langname{Java} compilers exist, most frequently Java programs are compiled into intermediate byte code and this is interpreted, and usually the interpreter includes a JIT compiler. For calling \langname{Java} functions or accessing Java objects from \Rlang code, the solution is to use package \pkgname{rJava}. One important point to remember is that the \langname{Java} SE Development Kit (JDK) must be installed for this package to work. The usually installed \langname{Java} runtime environment (JRE) is not enough.

We need first to start the Java Virtual Machine (the byte-code interpreter).

<<rjava-01>>=
.jinit()
@

The code that follows is not that clear, and merits some explanation.

We first create a \langname{Java} array from inside R.

<<rjava-02>>=
a <- .jarray( list(
                   .jnew( "java/awt/Point", 10L, 10L ),
                   .jnew( "java/awt/Point", 30L, 30L )
                   ) )
print(a)
mode(a)
class(a)
str(a)
@

Then we use base \Rlang function \Rfunction{lapply()} to apply a user-defined \Rlang function to the elements of the Java array, obtaining as returned value an \Rlang array.

<<rjava-03>>=
b <- sapply(a,
            function(point){
              with(point, {
                (x + y )^2
              } )
            })
print(b)
mode(b)
class(b)
str(b)
@

Although more cumbersome than in the case of \pkgname{Rcpp} one can manually write wrapper code to hide the special syntax and object types from users.

It is also possible to call \Rlang functions from within a \langname{Java} program. This is outside the scope of this book.

\subsection{sh, bash}

The\index{command shell}\pgrmname{sh}\pgrmname{bash} operating system shell can be accessed from within \Rlang and the output from programs and shell scripts returned to the \Rlang session. This is useful, for example for pre-processing raw data files with tools like \langname{AWK} or \langname{Perl} scripts. The problem with this approach is that when it is used, the \Rlang script cannot run portably across operating systems, or in the absence of the tools or sh or bash scripts. Except for code that will never be reused (i.e.\ it is used once and discarded) it is preferable to use \Rlang  built-in commands whenever possible, or if shell scripts are used, to make the shell script the master script from within which the \Rlang scripts are called, rather than the other way around. The reason for this is mainly making clear the developer's intention: that the code as a whole will be run in a given operating system using a certain set of tools, rather hiding shell calls inside the \Rlang script. In other words, keep the least portable bits in full view.

\section{Web pages, and interactive interfaces}

There is a lot that could be written on using \Rlang to create web pages, interactive widgets and gadgets for both local and remote user interaction. This is an area currently under intense development. One example is the \pkgname{Shiny} package and Shiny server \url{https://shiny.rstudio.com/}. This package allows the creation of interactive displays to be viewed through any web browser.

\sloppy
There are other packages for generating both static and interactive graphics in formats suitable for on-line display, as well as package \pkgname{knitr} \url{https://yihui.name/knitr/} used for writing the present book, which when using \Rlang Markdown for markup (with package \pkgname{rmarkdown} \url{http://rmarkdown.rstudio.com} or \pkgname{Bookdown} \url{https://bookdown.org/} can output self-contained HTML files in addition to RTF and PDF formats.

\section{A final thought on package writing}

Some \Rlang packages provide some fundamentally new capabilities, but many packages provide new user interfaces to functionality that already exists in base \Rlang or contributed packages. In many cases the improved usability provided by a well thought and consistent user interface can make packages that are to a large extent wrappers on existing functions and methods still worthwhile developing.

If you attempt to improve or extend existing functionality, do study existing code. \Rlang itself and all packages archived in CRAN are open-source and published under licences that allow re-use and modification as long you acknowledge all authors and abide to the requirement of releasing your version under the same or similar licences as the original code.

If you study the code of package \pkgname{ggpmisc}, you will quickly realize that for writing many of the methods and functions I have used code from package \pkgname{ggplot2} as a template, building upon it with new code that calls functions defined in base \Rlang and very frequently also in other packages. Of course, other packages may do the opposite: to respect the user interface of existing functions but provide improved performance.

Some of the packages in the \pkgname{tidyverse} developed by Hadley Wickham and collaborators evolved so that early versions focused on an improved user interaction (improved grammars) and only later performance was honed until it became as good or better than that of the earlier alternatives. The development of \pkgname{data.table} seems to have started by focusing on performance, and much of the unusual user interface reflects this. The packages in the \pkgname{tidyverse} are becoming very popular, while \pkgname{data.table} remains a specialized tool for cases where performance is extremely important.

When developing \Rlang packages, including a good coverage of test cases as part of the package itself simplifies code maintenance enormously, helps in maintaining consistency of behaviour across versions, and reveals in good time problems triggered by updates to \Rpgrm or packages depended upon.

So, my recommendation is to spend time designing and testing the user interface first. Go through design and test cycles, until you are sure the interface is user friendly, consistent and complete. Be consistent not only within your own code, but with \Rlang itself and/or popular packages. Avoid name clashes except when the functions or methods you develop are intended to directly replace those they make inaccessible. Be very careful with new names for methods, functions and operators, make sure they fully describe their function. If equivalent methods exist in \Rlang or in the packages your own code expands upon, if possible define method specializations of the existing methods for the new classes you define. Try to create as few new \emph{words} for the user to learn. However, keep the future in mind, making the user interface flexible and expandable. Even if you will be the only user, having a good interface will make it easier for you to remember how to use your own package.

<<>>=
try(detach(package:ggplot2))
try(detach(package:tibble))
try(detach(package:profr))
try(detach(package:microbenchmark))
# try(detach(package:rJava))
# try(detach(package:rPython))
try(detach(package:inline))
try(detach(package:Rcpp))
@

<<eval=eval_diag, include=eval_diag, echo=eval_diag, cache=FALSE>>=
knitter_diag()
R_diag()
other_diag()
@

